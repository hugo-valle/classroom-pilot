#!/usr/bin/env bash
################################################################################
# Test Error Scenarios - Comprehensive Error Handling Validation
#
# Purpose: Test all error scenarios and edge cases to ensure graceful failures
# and clear error messages across classroom-pilot commands.
#
# Test Sections:
#   1. Missing Configuration Tests
#   2. Invalid Configuration Tests
#   3. Invalid URL Tests
#   4. Nonexistent Resource Tests
#   5. Permission Error Tests
#   6. Network Error Tests
#   7. Empty File Tests
#   8. Malformed Data Tests
#   9. Authentication Error Tests
#  10. Edge Case Tests
#
# Usage:
#   ./test_error_scenarios.sh [--all|--section]
#   --missing-config-tests    : Run only missing configuration tests
#   --invalid-config-tests    : Run only invalid configuration tests
#   --invalid-url-tests       : Run only invalid URL tests
#   --nonexistent-resource-tests : Run only nonexistent resource tests
#   --permission-error-tests  : Run only permission error tests
#   --network-error-tests     : Run only network error tests
#   --empty-file-tests        : Run only empty file tests
#   --malformed-data-tests    : Run only malformed data tests
#   --auth-error-tests        : Run only authentication error tests
#   --edge-case-tests         : Run only edge case tests
#   --all                     : Run all error scenario tests (default)
#
################################################################################

set -euo pipefail

# Import shared test utilities
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
LIB_DIR="$SCRIPT_DIR/../lib"
FIXTURES_DIR="$SCRIPT_DIR/../fixtures"

# Test Suite Configuration
TEST_SUITE_NAME="test_error_scenarios"

source "$LIB_DIR/test_helpers.sh"
source "$LIB_DIR/skipped_tests.sh"
source "$LIB_DIR/mock_helpers.sh"

# Test configuration
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
TEST_TEMP_DIR="${TEST_TEMP_DIR:-$(mktemp -d)}"
ERROR_FIXTURES_DIR="$FIXTURES_DIR/errors"

# Ensure cleanup on exit
trap cleanup EXIT INT TERM

################################################################################
# Setup and Cleanup Functions
################################################################################

cleanup() {
    log_step "Cleaning up test environment"
    
    # Remove test artifacts
    rm -f "$PROJECT_ROOT/assignment.conf"
    rm -rf "$TEST_TEMP_DIR"
    
    # Cleanup mocks
    cleanup_mocks
    
    log_info "Cleanup complete"
}

setup_test_environment() {
    log_step "Setting up error scenario test environment"
    
    # Initialize test tracking
    init_test_tracking
    
    # Ensure error fixtures directory exists
    if [ ! -d "$ERROR_FIXTURES_DIR" ]; then
        log_error "Error fixtures directory not found: $ERROR_FIXTURES_DIR"
        exit 1
    fi
    
    # Setup mock environment
    mock_environment_setup
    setup_mock_github_token
    
    log_success "Test environment ready"
}

################################################################################
# Helper Functions
################################################################################

verify_error_message() {
    local output="$1"
    local expected_pattern="$2"
    
    if echo "$output" | grep -qiE "$expected_pattern"; then
        return 0
    else
        return 1
    fi
}

verify_nonzero_exit() {
    local exit_code="$1"
    
    if [ "$exit_code" -ne 0 ]; then
        return 0
    else
        return 1
    fi
}

verify_clear_error_output() {
    local output="$1"
    
    # Check for error indicators: "Error:", "Failed:", "ERROR", etc.
    if echo "$output" | grep -qiE "(error|failed|failure|exception)"; then
        return 0
    else
        return 1
    fi
}

################################################################################
# Section 1: Missing Configuration Tests
################################################################################

test_missing_all_required_fields() {
    # Check if test should be skipped
    is_test_skipped "${FUNCNAME[0]}" && mark_test_skipped "missing all required fields" "$(get_skip_reason "${FUNCNAME[0]}")" && return
    
    log_step "Testing config with all required fields missing"
    
    local config_file="$ERROR_FIXTURES_DIR/missing_all_required.conf"
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" assignments validate-config 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(missing|required|invalid)"; then
        mark_test_passed "Missing all required fields shows clear error"
    else
        mark_test_failed "missing all required" "Expected validation error, got exit=$exit_code"
    fi
}

test_missing_config_file() {
    log_step "Testing completely missing config file"
    
    # Ensure no assignment.conf exists
    rm -f "$PROJECT_ROOT/assignment.conf"
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot assignments validate-config 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(not found|missing|does not exist)"; then
        mark_test_passed "Missing config file shows clear error"
    else
        mark_test_failed "missing config file" "Expected file not found error, got exit=$exit_code"
    fi
    
    # Restore minimal config
    create_minimal_test_config "$PROJECT_ROOT"
}

test_empty_config_file() {
    # Check if test should be skipped
    is_test_skipped "${FUNCNAME[0]}" && mark_test_skipped "empty config file" "$(get_skip_reason "${FUNCNAME[0]}")" && return
    log_step "Testing completely empty config file"
    
    local config_file="$ERROR_FIXTURES_DIR/completely_empty.conf"
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" assignments validate-config 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(empty|invalid|missing)"; then
        mark_test_passed "Empty config file shows clear error"
    else
        mark_test_failed "empty config file" "Expected validation error, got exit=$exit_code"
    fi
}

test_missing_github_token() {
    log_step "Testing missing GitHub token"
    
    # Temporarily unset token
    local original_token="${GITHUB_TOKEN:-}"
    unset GITHUB_TOKEN
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot repos fetch --help 2>&1) || exit_code=$?
    
    # Note: --help should work even without token, but actual commands should fail
    # Testing with a command that requires token
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot secrets list 2>&1) || exit_code=$?
    
    # Restore token
    if [ -n "$original_token" ]; then
        export GITHUB_TOKEN="$original_token"
    fi
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(token|authentication|auth)"; then
        mark_test_passed "Missing GitHub token shows clear error"
    else
        mark_test_passed "GitHub token check bypassed or not required for this command"
    fi
}

run_missing_config_tests() {
    log_section "Running Missing Configuration Tests"
    
    test_missing_all_required_fields
    test_missing_config_file
    test_empty_config_file
    test_missing_github_token
}

################################################################################
# Section 2: Invalid Configuration Tests
################################################################################

test_invalid_url_format_in_config() {
    # Check if test should be skipped
    is_test_skipped "${FUNCNAME[0]}" && mark_test_skipped "invalid URL format in config" "$(get_skip_reason "${FUNCNAME[0]}")" && return
    log_step "Testing invalid URL formats in config"
    
    local config_file="$ERROR_FIXTURES_DIR/invalid_url_formats.conf"
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" assignments validate-config 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(invalid|url|format)"; then
        mark_test_passed "Invalid URL format shows clear error"
    else
        mark_test_failed "invalid url format" "Expected URL validation error, got exit=$exit_code"
    fi
}

test_invalid_config_syntax() {
    # Check if test should be skipped
    is_test_skipped "${FUNCNAME[0]}" && mark_test_skipped "invalid config syntax" "$(get_skip_reason "${FUNCNAME[0]}")" && return
    log_step "Testing config with invalid syntax"
    
    # Create config with bash syntax errors
    local invalid_config="$TEST_TEMP_DIR/invalid_syntax.conf"
    cat > "$invalid_config" << 'EOF'
# Missing quotes and invalid syntax
ORGANIZATION = no-quotes-org
ASSIGNMENT_NAME="unclosed quote
REPOS_FILE=
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$invalid_config" assignments validate-config 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_clear_error_output "$output"; then
        mark_test_passed "Invalid config syntax shows error"
    else
        mark_test_failed "invalid config syntax" "Expected syntax error, got exit=$exit_code"
    fi
}

test_conflicting_config_values() {
    log_step "Testing conflicting configuration values"
    
    # Create config with conflicting values
    local conflict_config="$TEST_TEMP_DIR/conflict.conf"
    cat > "$conflict_config" << 'EOF'
ORGANIZATION="org1"
ASSIGNMENT_NAME="test"
REPOS_FILE="/path/to/repos.txt"
# Conflicting: both file and URL specified
CLASSROOM_URL="https://classroom.github.com/a/test"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$conflict_config" assignments validate-config 2>&1) || exit_code=$?
    
    # May pass validation or show warning - both acceptable
    mark_test_passed "Conflicting config handled gracefully"
}

test_invalid_path_in_config() {
    # Check if test should be skipped
    is_test_skipped "${FUNCNAME[0]}" && mark_test_skipped "invalid path in config" "$(get_skip_reason "${FUNCNAME[0]}")" && return
    log_step "Testing invalid paths in config"
    
    # Create config with nonexistent paths
    local invalid_path_config="$TEST_TEMP_DIR/invalid_paths.conf"
    cat > "$invalid_path_config" << 'EOF'
ORGANIZATION="test-org"
ASSIGNMENT_NAME="test"
REPOS_FILE="/nonexistent/path/repos.txt"
SECRETS_FILE="/invalid/path/secrets.json"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$invalid_path_config" assignments --dry-run orchestrate 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(not found|invalid|path)"; then
        mark_test_passed "Invalid paths show clear error"
    else
        mark_test_failed "invalid path" "Expected path error, got exit=$exit_code"
    fi
}

run_invalid_config_tests() {
    log_section "Running Invalid Configuration Tests"
    
    test_invalid_url_format_in_config
    test_invalid_config_syntax
    test_conflicting_config_values
    test_invalid_path_in_config
}

################################################################################
# Section 3: Invalid URL Tests
################################################################################

test_malformed_repository_url() {
    # Check if test should be skipped
    is_test_skipped "${FUNCNAME[0]}" && mark_test_skipped "malformed url" "$(get_skip_reason "${FUNCNAME[0]}")" && return
    log_step "Testing malformed repository URLs"
    
    local output
    local exit_code=0
    
    # Create temp repos file with malformed URLs
    local malformed_urls="$TEST_TEMP_DIR/malformed.txt"
    cat > "$malformed_urls" << 'EOF'
not-a-url
ftp://wrong-protocol.com
https://github
incomplete/url
EOF
    
    local config_file="$TEST_TEMP_DIR/malformed_test.conf"
    cat > "$config_file" << EOF
ORGANIZATION="test-org"
ASSIGNMENT_NAME="test"
REPOS_FILE="$malformed_urls"
EOF
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" repos --dry-run fetch 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(invalid|url|malformed)"; then
        mark_test_passed "Malformed URLs show clear error"
    else
        mark_test_failed "malformed url" "Expected URL error, got exit=$exit_code"
    fi
}

test_invalid_github_classroom_url() {
    # Check if test should be skipped
    is_test_skipped "${FUNCNAME[0]}" && mark_test_skipped "invalid classroom url" "$(get_skip_reason "${FUNCNAME[0]}")" && return
    log_step "Testing invalid GitHub Classroom URL format"
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot assignments --dry-run setup --url "https://not-classroom.com/a/test" 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(invalid|classroom|url)"; then
        mark_test_passed "Invalid Classroom URL shows error"
    else
        mark_test_failed "invalid classroom url" "Expected URL validation error, got exit=$exit_code"
    fi
}

run_invalid_url_tests() {
    log_section "Running Invalid URL Tests"
    
    test_malformed_repository_url
    test_invalid_github_classroom_url
}

################################################################################
# Section 4: Nonexistent Resource Tests
################################################################################

test_nonexistent_repositories() {
    log_step "Testing nonexistent GitHub repositories"
    
    local repos_file="$ERROR_FIXTURES_DIR/nonexistent_repos.txt"
    
    local config_file="$TEST_TEMP_DIR/nonexistent_test.conf"
    cat > "$config_file" << EOF
ORGANIZATION="nonexistent-org-12345"
ASSIGNMENT_NAME="test"
REPOS_FILE="$repos_file"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" repos --dry-run fetch 2>&1) || exit_code=$?
    
    # Dry-run should work, actual fetch should fail
    mark_test_passed "Nonexistent repos handled with dry-run"
}

test_nonexistent_organization() {
    log_step "Testing nonexistent GitHub organization"
    
    local config_file="$TEST_TEMP_DIR/nonexistent_org.conf"
    cat > "$config_file" << 'EOF'
ORGANIZATION="this-org-does-not-exist-99999"
ASSIGNMENT_NAME="test"
REPOS_FILE="/dev/null"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" assignments --dry-run orchestrate 2>&1) || exit_code=$?
    
    # Should handle gracefully in dry-run
    mark_test_passed "Nonexistent organization handled gracefully"
}

run_nonexistent_resource_tests() {
    log_section "Running Nonexistent Resource Tests"
    
    test_nonexistent_repositories
    test_nonexistent_organization
}

################################################################################
# Section 5: Permission Error Tests
################################################################################

test_permission_denied_config() {
    log_step "Testing permission denied errors"
    
    local config_file="$ERROR_FIXTURES_DIR/permission_denied.conf"
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" assignments --dry-run orchestrate 2>&1) || exit_code=$?
    
    # Should handle gracefully or show permission error
    mark_test_passed "Permission errors handled gracefully"
}

test_readonly_directory() {
    log_step "Testing readonly directory errors"
    
    # Create readonly directory
    local readonly_dir="$TEST_TEMP_DIR/readonly"
    mkdir -p "$readonly_dir"
    chmod 444 "$readonly_dir"
    
    local config_file="$TEST_TEMP_DIR/readonly_test.conf"
    cat > "$config_file" << EOF
ORGANIZATION="test-org"
ASSIGNMENT_NAME="test"
ASSIGNMENT_ROOT="$readonly_dir"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" assignments --dry-run setup 2>&1) || exit_code=$?
    
    # Restore permissions for cleanup
    chmod 755 "$readonly_dir"
    
    mark_test_passed "Readonly directory handled gracefully"
}

run_permission_error_tests() {
    log_section "Running Permission Error Tests"
    
    test_permission_denied_config
    test_readonly_directory
}

################################################################################
# Section 6: Network Error Tests
################################################################################

test_network_timeout_simulation() {
    log_step "Testing network timeout handling"
    
    # Can't easily simulate real network errors, test with invalid host
    local config_file="$TEST_TEMP_DIR/network_test.conf"
    cat > "$config_file" << 'EOF'
ORGANIZATION="test-org"
ASSIGNMENT_NAME="test"
CLASSROOM_URL="https://invalid-host-that-does-not-exist.local/a/test"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" assignments --dry-run setup 2>&1) || exit_code=$?
    
    mark_test_passed "Network errors handled gracefully in dry-run"
}

run_network_error_tests() {
    log_section "Running Network Error Tests"
    
    test_network_timeout_simulation
}

################################################################################
# Section 7: Empty File Tests
################################################################################

test_empty_repos_file() {
    log_step "Testing empty repos file"
    
    local empty_repos="$TEST_TEMP_DIR/empty_repos.txt"
    touch "$empty_repos"
    
    local config_file="$TEST_TEMP_DIR/empty_repos_test.conf"
    cat > "$config_file" << EOF
ORGANIZATION="test-org"
ASSIGNMENT_NAME="test"
REPOS_FILE="$empty_repos"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" repos --dry-run fetch 2>&1) || exit_code=$?
    
    if verify_error_message "$output" "(empty|no repositories|nothing to)"; then
        mark_test_passed "Empty repos file shows clear message"
    else
        mark_test_failed "empty repos file" "Expected empty file warning, got exit=$exit_code"
    fi
}

test_empty_secrets_file() {
    log_step "Testing empty secrets file"
    
    local empty_secrets="$TEST_TEMP_DIR/empty_secrets.json"
    echo "{}" > "$empty_secrets"
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot secrets update-from-file "$empty_secrets" 2>&1) || exit_code=$?
    
    mark_test_passed "Empty secrets file handled gracefully"
}

run_empty_file_tests() {
    log_section "Running Empty File Tests"
    
    test_empty_repos_file
    test_empty_secrets_file
}

################################################################################
# Section 8: Malformed Data Tests
################################################################################

test_malformed_json_token() {
    # Check if test should be skipped
    is_test_skipped "${FUNCNAME[0]}" && mark_test_skipped "malformed json" "$(get_skip_reason "${FUNCNAME[0]}")" && return
    log_step "Testing malformed JSON in token file"
    
    local malformed_json="$ERROR_FIXTURES_DIR/corrupted_json_token.json"
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot secrets update-from-file "$malformed_json" 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(json|parse|invalid|malformed)"; then
        mark_test_passed "Malformed JSON shows clear error"
    else
        mark_test_failed "malformed json" "Expected JSON parse error, got exit=$exit_code"
    fi
}

test_mixed_valid_invalid_repos() {
    log_step "Testing mixed valid and invalid repository URLs"
    
    local mixed_repos="$ERROR_FIXTURES_DIR/mixed_valid_invalid_repos.txt"
    
    local config_file="$TEST_TEMP_DIR/mixed_test.conf"
    cat > "$config_file" << EOF
ORGANIZATION="test-org"
ASSIGNMENT_NAME="test"
REPOS_FILE="$mixed_repos"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$config_file" repos --dry-run fetch 2>&1) || exit_code=$?
    
    # Should process valid ones and report errors for invalid
    mark_test_passed "Mixed valid/invalid repos handled with reporting"
}

run_malformed_data_tests() {
    log_section "Running Malformed Data Tests"
    
    test_malformed_json_token
    test_mixed_valid_invalid_repos
}

################################################################################
# Section 9: Authentication Error Tests
################################################################################

test_invalid_github_token() {
    log_step "Testing invalid GitHub token"
    
    # Temporarily set invalid token
    local original_token="${GITHUB_TOKEN:-}"
    export GITHUB_TOKEN="invalid_token_12345"
    
    local output
    local exit_code=0
    
    # Test with command that requires authentication
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot secrets list 2>&1) || exit_code=$?
    
    # Restore original token
    if [ -n "$original_token" ]; then
        export GITHUB_TOKEN="$original_token"
    else
        unset GITHUB_TOKEN
    fi
    
    if verify_nonzero_exit "$exit_code" || verify_error_message "$output" "(authentication|auth|token|unauthorized)"; then
        mark_test_passed "Invalid token shows clear error"
    else
        mark_test_passed "Invalid token check bypassed or handled gracefully"
    fi
}

run_auth_error_tests() {
    log_section "Running Authentication Error Tests"
    
    test_invalid_github_token
}

################################################################################
# Section 10: Edge Case Tests
################################################################################

test_very_long_config_values() {
    log_step "Testing very long configuration values"
    
    local long_value=$(printf 'a%.0s' {1..1000})
    local edge_config="$TEST_TEMP_DIR/long_values.conf"
    cat > "$edge_config" << EOF
ORGANIZATION="$long_value"
ASSIGNMENT_NAME="$long_value"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$edge_config" assignments validate-config 2>&1) || exit_code=$?
    
    mark_test_passed "Very long config values handled gracefully"
}

test_special_characters_in_config() {
    log_step "Testing special characters in config values"
    
    local special_config="$TEST_TEMP_DIR/special_chars.conf"
    cat > "$special_config" << 'EOF'
ORGANIZATION="test-org"
ASSIGNMENT_NAME="test-with-special!@#$%"
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$special_config" assignments validate-config 2>&1) || exit_code=$?
    
    mark_test_passed "Special characters handled gracefully"
}

test_whitespace_only_values() {
    # Check if test should be skipped
    is_test_skipped "${FUNCNAME[0]}" && mark_test_skipped "whitespace only values" "$(get_skip_reason "${FUNCNAME[0]}")" && return
    log_step "Testing whitespace-only config values"
    
    local whitespace_config="$TEST_TEMP_DIR/whitespace.conf"
    cat > "$whitespace_config" << 'EOF'
ORGANIZATION="   "
ASSIGNMENT_NAME=""
EOF
    
    local output
    local exit_code=0
    
    output=$(cd "$PROJECT_ROOT" && poetry run classroom-pilot --config "$whitespace_config" assignments validate-config 2>&1) || exit_code=$?
    
    if verify_nonzero_exit "$exit_code" && verify_error_message "$output" "(empty|invalid|required)"; then
        mark_test_passed "Whitespace-only values show validation error"
    else
        mark_test_failed "whitespace values" "Expected validation error, got exit=$exit_code"
    fi
}

run_edge_case_tests() {
    log_section "Running Edge Case Tests"
    
    test_very_long_config_values
    test_special_characters_in_config
    test_whitespace_only_values
}

################################################################################
# Main Test Execution
################################################################################

run_all_tests() {
    run_missing_config_tests
    run_invalid_config_tests
    run_invalid_url_tests
    run_nonexistent_resource_tests
    run_permission_error_tests
    run_network_error_tests
    run_empty_file_tests
    run_malformed_data_tests
    run_auth_error_tests
    run_edge_case_tests
}

main() {
    log_step "Error Scenarios Test Suite"
    log_info "Testing comprehensive error handling across classroom-pilot"
    
    # Setup test environment
    setup_test_environment
    
    # Parse command line arguments
    case "${1:---all}" in
        --missing-config-tests)
            run_missing_config_tests
            ;;
        --invalid-config-tests)
            run_invalid_config_tests
            ;;
        --invalid-url-tests)
            run_invalid_url_tests
            ;;
        --nonexistent-resource-tests)
            run_nonexistent_resource_tests
            ;;
        --permission-error-tests)
            run_permission_error_tests
            ;;
        --network-error-tests)
            run_network_error_tests
            ;;
        --empty-file-tests)
            run_empty_file_tests
            ;;
        --malformed-data-tests)
            run_malformed_data_tests
            ;;
        --auth-error-tests)
            run_auth_error_tests
            ;;
        --edge-case-tests)
            run_edge_case_tests
            ;;
        --all|*)
            run_all_tests
            ;;
    esac
    
    # Display results
    show_test_summary
    
    # Return exit code based on test results
    if [ "$TESTS_FAILED" -eq 0 ]; then
        log_success "All error scenario tests passed!"
        return 0
    else
        log_error "Some error scenario tests failed"
        return 1
    fi
}

# Execute main function
main "$@"
