name: Performance Monitoring

on:
  push:
    branches: [main, develop]
  schedule:
    # Weekly performance monitoring every Sunday at 02:00 UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      baseline_reset:
        description: 'Reset performance baseline'
        required: false
        default: false
        type: boolean
      performance_threshold:
        description: 'Performance regression threshold (%)'
        required: false
        default: '20'
        type: string

env:
  BASELINE_THRESHOLD: ${{ github.event.inputs.performance_threshold || '20' }}
  RESET_BASELINE: ${{ github.event.inputs.baseline_reset || 'false' }}

jobs:
  python-cli-performance-benchmarking:
    name: Python CLI Performance Benchmarking
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      matrix:
        scenario:
          - name: "small-repo-set"
            repo_count: "5"
            config_type: "basic"
          - name: "large-repo-set"
            repo_count: "25"
            config_type: "advanced"
          - name: "enterprise-repo-set"
            repo_count: "50"
            config_type: "enterprise"
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          version: latest
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Install Dependencies
        run: poetry install

      - name: Setup Performance Environment
        run: |
          # Install performance monitoring tools
          sudo apt-get update
          sudo apt-get install -y time bc jq
          
          # Create performance test directories
          mkdir -p performance_test_data
          mkdir -p performance_results
          
          # Setup test assignment configuration
          cp assignment.conf performance_test_data/test_assignment.conf

      - name: Load Workflow Utilities
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          echo "Loaded workflow utilities for performance monitoring"

      - name: Load Previous Performance Baseline
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          if [ "$RESET_BASELINE" = "false" ]; then
            load_baseline "${{ matrix.scenario.name }}" || echo "No previous baseline found"
          else
            echo "Resetting baseline as requested"
          fi

      - name: Benchmark Assignment Orchestrator (Python CLI)
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          start_step_timing "assignment_orchestrator_benchmark"
          
          # Create test configuration for this scenario
          sed -i "s/MAX_REPOS=.*/MAX_REPOS=${{ matrix.scenario.repo_count }}/" performance_test_data/test_assignment.conf
          
          # Benchmark Python CLI assignment orchestrator with dry-run
          benchmark_script "assignments-orchestrate" \
            "python -m classroom_pilot assignments --dry-run --verbose orchestrate" \
            "--config performance_test_data/test_assignment.conf"
          
          end_step_timing "assignment_orchestrator_benchmark"

      - name: Benchmark Repository Fetch (Python CLI)
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          start_step_timing "repo_fetch_benchmark"
          
          # Benchmark Python CLI repo fetch with dry-run
          benchmark_script "repos-fetch" \
            "python -m classroom_pilot repos --dry-run --verbose fetch" \
            "--config performance_test_data/test_assignment.conf"
          
          end_step_timing "repo_fetch_benchmark"

      - name: Benchmark Secrets Management (Python CLI)
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          start_step_timing "secrets_benchmark"
          
          # Benchmark Python CLI secrets management with dry-run
          benchmark_script "secrets-add" \
            "python -m classroom_pilot secrets --dry-run --verbose add" \
            "--assignment-root performance_test_data/"
          
          end_step_timing "secrets_benchmark"

      - name: Monitor Resource Usage (Python CLI)
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          start_step_timing "resource_monitoring"
          
          # Monitor system resources during Python CLI execution
          monitor_resources "python_cli_workflow_simulation" \
            "python -m classroom_pilot assignments --dry-run --verbose orchestrate --config performance_test_data/test_assignment.conf"
          
          end_step_timing "resource_monitoring"

      - name: Detect Performance Regressions
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          start_step_timing "regression_detection"
          
          # Compare current performance against baseline
          if detect_regression "${{ matrix.scenario.name }}" "$BASELINE_THRESHOLD"; then
            echo "✅ Performance within acceptable thresholds"
          else
            echo "⚠️ Performance regression detected above $BASELINE_THRESHOLD% threshold"
            exit 1
          fi
          
          end_step_timing "regression_detection"

      - name: Store Performance Baseline
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          # Store new baseline for this scenario
          store_baseline "${{ matrix.scenario.name }}"

      - name: Generate Performance Report
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          # Generate comprehensive performance report
          generate_performance_report "${{ matrix.scenario.name }}" "performance_results/"
          
          # Export performance metrics for aggregation
          export_performance_metrics "script_performance" "performance_results/metrics.json"

      - name: Upload Performance Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: script-performance-${{ matrix.scenario.name }}
          path: |
            performance_results/
            performance_test_data/
          retention-days: 30

  cli-performance-testing:
    name: CLI Performance Testing
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: python-cli-performance-benchmarking
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install poetry
          poetry install

      - name: Load Performance Utilities
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          echo "Loaded utilities for CLI performance testing"

      - name: Benchmark CLI Assignment Commands
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          start_step_timing "cli_assignments_benchmark"
          
          # Benchmark assignment orchestration CLI command
          benchmark_script "cli_assignments_orchestrate" \
            "poetry run classroom-pilot assignments orchestrate" \
            "--dry-run --config assignment.conf"
          
          # Benchmark assignment setup CLI command
          benchmark_script "cli_assignments_setup" \
            "poetry run classroom-pilot assignments setup" \
            "--dry-run --config assignment.conf"
          
          end_step_timing "cli_assignments_benchmark"

      - name: Benchmark CLI Repository Commands
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          start_step_timing "cli_repos_benchmark"
          
          # Benchmark repository fetch CLI command
          benchmark_script "cli_repos_fetch" \
            "poetry run classroom-pilot repos fetch" \
            "--dry-run --config assignment.conf"
          
          # Benchmark collaborator management CLI command
          benchmark_script "cli_repos_collaborator" \
            "poetry run classroom-pilot repos collaborator" \
            "--dry-run --config assignment.conf"
          
          end_step_timing "cli_repos_benchmark"

      - name: Benchmark CLI Secrets Commands
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          start_step_timing "cli_secrets_benchmark"
          
          # Benchmark secrets management CLI command
          benchmark_script "cli_secrets_manage" \
            "poetry run classroom-pilot secrets manage" \
            "--dry-run --config assignment.conf"
          
          end_step_timing "cli_secrets_benchmark"

      - name: Generate CLI Performance Report
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          # Generate CLI-specific performance report
          generate_performance_report "cli_performance" "cli_results/"
          
          # Export CLI performance metrics
          export_performance_metrics "cli_performance" "cli_results/metrics.json"

      - name: Upload CLI Performance Artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: cli-performance-results
          path: cli_results/
          retention-days: 30

  performance-trend-analysis:
    name: Performance Trend Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [python-cli-performance-benchmarking, cli-performance-testing]
    if: always()
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download All Performance Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: '*-performance-*'
          path: all_performance_results/
          merge-multiple: true

      - name: Load Performance Utilities
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          echo "Loaded utilities for trend analysis"

      - name: Aggregate Performance Metrics
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          start_step_timing "metrics_aggregation"
          
          # Aggregate all performance metrics
          aggregate_performance_metrics "all_performance_results/" "aggregated_results/"
          
          end_step_timing "metrics_aggregation"

      - name: Generate Comprehensive Performance Dashboard
        run: |
          source .github/scripts/workflow_utils.sh
          source .github/scripts/performance_utils.sh
          
          # Create comprehensive performance dashboard
          create_step_summary "performance-monitoring" \
            "Performance Monitoring Results" \
            "aggregated_results/dashboard.md"
          
          # Export final aggregated metrics
          export_performance_metrics "performance_monitoring" "aggregated_results/final_metrics.json"

      - name: Upload Final Performance Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-monitoring-final
          path: aggregated_results/
          retention-days: 90